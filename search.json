[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ceren Unal",
    "section": "",
    "text": "Ceren Unal is a digital marketing expert specializing in digital products and services. When not innovating on advertising platforms, Ceren enjoys exploring the LA food scene, reading and watching plays, visiting museums and learning about art and photography.\nEducation\nCal Poly Pomona | MS in Digital Marketing Analytics | Aug 2024 - Aug 2026\nBoğaziçi University | BA in English Literature | Aug 2017 - May 2022"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "",
    "text": "This study aims to develop a customer churn prediction model for the fictional streaming service Skystream, using a synthetic dataset.\nIn Part 1, I developed a benchmark churn prediction model for Skystream using the K-Nearest Neighbors (KNN) algorithm. The KNN model correctly identified 47% of all churn cases, achieving a precision of 57%.\nIn Part 2 the goal is to build a Random Forest model that outperforms KNN on imbalanced, high-dimensional data, with a focus on improving both recall and precision."
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html#load-packages-data",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html#load-packages-data",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "\n1 Load Packages & Data",
    "text": "1 Load Packages & Data\nWe will be using the Tidyverse package to process our data set. As in Part 1, I will clean up the incorrect namings in the Genre variable and create a Churn variable based on CancelDate.\n\nCodelibrary(tidyverse)\nlibrary(DataExplorer)\n\nskystream &lt;- read_csv(file = \"skystream.csv\")\n\nskystream &lt;- skystream|&gt; \n  mutate(\n    churn = if_else(is.na(CancelDate), 0L, 1L) #return 1 if there is a CancelDate\n  ) %&gt;%\n  relocate(churn, .after = 1) |&gt;  #position it after column 1 \n  mutate(Genre = recode(Genre, \"Dramas\" = \"Drama\")) #replace Dramas with Drama\n\nhead(skystream)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomerID\nchurn\nGenre\nState\nSubscriptionTier\nAge\nGender\nJoinDate\nCancelDate\nNumberMonthsActive\nAvgSessionLength\nMonthsSinceLastActivity\nAvgWatchHoursPerMonth\nRevenueYTD\nDevicePreference\n\n\n\n8\n0\nHorror\nIL\nPremium\n42\nM\n2025-01-29\nNA\n12\n55\n0\n65\n191.88\nMobile\n\n\n2\n0\nAction & Adventure\nTX\nFamily\n39\nF\n2025-01-04\nNA\n12\n70\n0\n58\n239.88\nSmart TV\n\n\n59\n0\nDocumentary\nFL\nFamily\n48\nM\n2025-01-03\nNA\n12\n88\n0\n57\n239.88\nSmart TV\n\n\n11\n0\nAction & Adventure\nTX\nFamily\n30\nO\n2025-01-06\nNA\n12\n74\n0\n55\n239.88\nSmart TV\n\n\n75\n0\nHorror\nIL\nFamily\n28\nM\n2025-01-06\nNA\n12\n90\n0\n55\n239.88\nTablet\n\n\n35\n0\nChildren & Family\nGA\nFamily\n41\nM\n2025-01-02\nNA\n12\n88\n0\n55\n239.88\nMobile"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html#data-preparation-for-random-forest",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html#data-preparation-for-random-forest",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "\n2 Data Preparation for Random Forest",
    "text": "2 Data Preparation for Random Forest\nWe’ll prepare our data for Random Forest by ensuring that all continuous variables are numeric and we drop the variables that we won’t be using in our model. RevenueYTD is dropped as it would be reduntant when we have both SubscriptionTier and NumberMonthsActive in the model, which directly determine this 3rd variable.\nWe’ll convert the Churn variable, which indicates whether a customer has churned, into a factor so it can be treated as a categorical classification outcome.\nThe categorical variables will be recoded as dummy variables.\n\nCodelibrary(fastDummies)\nlibrary(snakecase)\n\n# Define columns by role/type\nnum_cols  &lt;- c(\"NumberMonthsActive\", \"AvgSessionLength\", \"Age\",\n               \"MonthsSinceLastActivity\", \"AvgWatchHoursPerMonth\")\n\ncat_cols  &lt;- c(\"SubscriptionTier\", \"Genre\", \"State\", \"Gender\", \"DevicePreference\")\n\nskystream_rf &lt;- skystream |&gt; \n  #Make “Churned” the first level so tidymodels treats Churned as the positive class\n  mutate(churn = factor(churn, levels = c(1, 0), labels = c(\"Churned\", \"Active\"))) |&gt; \n  \n  # Ensure numericals are numeric\n   mutate(across(all_of(num_cols), as.numeric)) |&gt; \n  \n  # Remove columns not suitable as KNN features \n  select(-CustomerID, -JoinDate, -CancelDate, -RevenueYTD) |&gt; \n  \n  # One-hot encode categoricals (drop first level to avoid redundancy)\n  fastDummies::dummy_cols(\n    select_columns = cat_cols,\n    remove_first_dummy = TRUE,\n    remove_selected_columns = TRUE\n  ) %&gt;%\n  \n  #Ensure column names are all in tidy format\n   rename_with(~ to_any_case(., case = \"upper_camel\"))"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html#creating-the-model",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html#creating-the-model",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "\n3 Creating the Model",
    "text": "3 Creating the Model\nWe set the seed to 867, the same as KNN, to ensure the observations are random and consistent across both models. We use initial_split() to divide the dataset, with 80% allocated for training and 20% for testing. Then, we use the strata = churned argument to ensure the class distribution of the churned variable is kept in both the training and testing sets. Finally, we extract the datasets for training and testing using split.\n\nCodelibrary(tidymodels)\nlibrary(themis)\n\nset.seed(867)\nSplit80  &lt;- initial_split(skystream_rf, prop = 0.80, strata = Churn)\n\nDataTrain &lt;- training(Split80) \nDataTest  &lt;- testing(Split80)  \n\nhead(DataTrain)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChurn\nAge\nNumberMonthsActive\nAvgSessionLength\nMonthsSinceLastActivity\nAvgWatchHoursPerMonth\nSubscriptionTierBasic\nSubscriptionTierFamily\nSubscriptionTierPremium\nGenreChildrenFamily\nGenreComedy\nGenreDocumentary\nGenreDrama\nGenreHorror\nStateCa\nStateCo\nStateFl\nStateGa\nStateIl\nStateNy\nStateTx\nStateWa\nGenderM\nGenderO\nDevicePreferenceMixed\nDevicePreferenceMobile\nDevicePreferenceSmartTv\nDevicePreferenceTablet\n\n\n\nActive\n42\n12\n55\n0\n65\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\nActive\n39\n12\n70\n0\n58\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\nActive\n48\n12\n88\n0\n57\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\nActive\n30\n12\n74\n0\n55\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nActive\n28\n12\n90\n0\n55\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n\n\nActive\n41\n12\n88\n0\n55\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n\n\n\n\nAs with KNN, we specify the recipe with Churn ~ ., which means all other columns, such as Age and NumberMonthsActive, will be used to predict whether a customer churned.\n\nCodeRecipeRF &lt;- recipe(Churn~., data=DataTrain)\n\n\nThe model design specifies min_n and mtry as tunable hyperparameters and sets the number of trees to 2,000 for increased stability. The model is configured to run using the \"ranger\" engine and the importance = \"impurity\" argument enables the calculation of variable importance scores, helping identify which features contribute most to churn prediction. Setting probability = TRUE ensures that class probabilities are returned (not just hard predictions), which is required for ROC AUC and threshold optimization. Finally, num.threads = detectCores() allows the model to use all available processor cores to speed up computation, and set_mode(\"classification\") confirms the task is binary classification (churn vs. active).\n\nCodelibrary(parallel)\nModelDesignRandFor &lt;- rand_forest(\n  min_n=tune(), \n  mtry=tune(), \n  trees=2000) |&gt; \n  set_engine(\"ranger\", \n             importance = \"impurity\", #generate variable importance plots\n             probability = TRUE, #generate class probabilities\n             num.threads=detectCores()) |&gt; #set as number of cores of the executing computer\n  set_mode(\"classification\")\n\nset.seed(123)\nWfModelRF=workflow() |&gt;                             \n           add_recipe(RecipeRF) |&gt; \n           add_model(ModelDesignRandFor) \n\n\nWe’ll tune min_n and mtry using a regular tuning grid created with grid_regular(), which generates 36 combinations of mtry (number of predictors sampled at each tree split) and min_n (minimum node size), evenly spaced across defined ranges. The finalize(mtry(), ...) function ensures that mtry adapts to the actual number of predictors in the training data.\nCross-validation is set up using vfold_cv() with 5 folds, stratified by the target variable Churn to preserve class balance in each fold. The tune_grid() function evaluates each hyperparameter combination using cross-validation and saves the class probabilities for later threshold tuning. A comprehensive metric set is used—including ROC AUC, accuracy, sensitivity, specificity, and F1 score—to assess both overall and class-specific performance.\n\nCoderf_grid &lt;- grid_regular(\n  finalize(mtry(), DataTrain |&gt; select(-Churn)),  # mtry depends on predictors\n  min_n(range = c(2L, 20L)),\n  levels = c(6, 6)   #36 configs\n)\n\nset.seed(123)\n\nFoldsForTuningRF=vfold_cv(DataTrain, v=5, strata=Churn)\n\nrf_tune &lt;- tune_grid(\n  WfModelRF,\n  resamples = FoldsForTuningRF,\n  grid = rf_grid,\n  metrics = metric_set(roc_auc, accuracy, sens, spec, f_meas),\n  control = control_grid(save_pred = TRUE)\n)\n\nautoplot(rf_tune)\n\n\n\n\n\n\n\nThe best hyperparameters are 16 for mtry and 9 for min_n , based on the ROC AUC metric finds the optimal balance between true positive rate and false positive rate.\n\nCodebest_rf &lt;- select_best(rf_tune, metric = \"roc_auc\")\n\nbest_rf\n\n\n\n\nmtry\nmin_n\n.config\n\n\n11\n9\nPreprocessor1_Model15\n\n\n\n\n\nAfter running the best hyper parameters, we go ahead and take a look at our confusion matrix to assess model performance.\n\nCodeWFModelBest=WfModelRF |&gt;\n  finalize_workflow(best_rf) |&gt;\n  fit(DataTrain)\n\nPredictionBestModel=augment(WFModelBest, DataTest)\n\ncm_rf &lt;- conf_mat(PredictionBestModel, truth=Churn,\n         estimate=.pred_class)\n\nlibrary(yardstick)\n\ncm_rf\n\n          Truth\nPrediction Churned Active\n   Churned       9      1\n   Active        8    162\n\nCodecm_rf %&gt;% summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.9500000\n\n\nkap\nbinary\n0.6415929\n\n\nsens\nbinary\n0.5294118\n\n\nspec\nbinary\n0.9938650\n\n\nppv\nbinary\n0.9000000\n\n\nnpv\nbinary\n0.9529412\n\n\nmcc\nbinary\n0.6680751\n\n\nj_index\nbinary\n0.5232768\n\n\nbal_accuracy\nbinary\n0.7616384\n\n\ndetection_prevalence\nbinary\n0.0555556\n\n\nprecision\nbinary\n0.9000000\n\n\nrecall\nbinary\n0.5294118\n\n\nf_meas\nbinary\n0.6666667\n\n\n\n\n\n\nRandom Forest is already outperforming KNN, with 86% recall and 86% precision, compared to 47% recall and 57% precision.\nVariable importance in our model reveals that MonthsSinceLastActivity was the biggest predictor of Churn.\n\nCodelibrary(vip)\nrf_fit &lt;- WFModelBest |&gt;\n          extract_fit_parsnip()\nvip(rf_fit$fit, num_features = 25)"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html#refining-the-model",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html#refining-the-model",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "\n4 Refining the Model",
    "text": "4 Refining the Model\nThresholds\nTo further improve the recall and precision of our model, we’ll try adjusting our threshold.\nAfter scanning a range of thresholds we find that the range of values from 0.47 to 0.5 maximizes F1 score, so the default threshold that we initially tuned our model to already maximized F1 score.\nThis is evident in that when we drop the threshold to 0.48, the recall and precision (along with all other metrics) remain the same.\n\nCode# Try thresholds from 0.05 to 0.95\nths &lt;- seq(0.05, 0.95, by = 0.01)\n\nscan_f1 &lt;- map_dfr(ths, function(t) {\n  cls &lt;- factor(\n    ifelse(PredictionBestModel$.pred_Churned &gt;= t, \"Churned\", \"Active\"),\n    levels = c(\"Churned\",\"Active\")\n  )\n  tibble(\n    threshold = t,\n    f1 = f_meas_vec(PredictionBestModel$Churn, cls, beta = 1),\n    precision = precision_vec(PredictionBestModel$Churn, cls),\n    recall = sens_vec(PredictionBestModel$Churn, cls)\n  )\n})\n\n# Show the best threshold by F1\nbest_thresh_f1 &lt;- scan_f1 |&gt;\n  slice_max(f1, n = 1)  |&gt;\n  pull(threshold)\n\nbest_thresh_f1 \n\n[1] 0.25 0.26 0.27\n\nCodePredictionBestModel_f1 &lt;- PredictionBestModel  |&gt;\n  mutate(\n    .pred_class_opt = factor(\n      ifelse(.pred_Churned &gt;= 0.48, \"Churned\", \"Active\"),\n      levels = c(\"Churned\",\"Active\")\n    )\n  )\n\ncm_rf_f1 &lt;- conf_mat(PredictionBestModel_f1, truth = Churn, estimate = .pred_class_opt)\n\ncm_rf_f1  |&gt;\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.9444444\n\n\nkap\nbinary\n0.6369504\n\n\nsens\nbinary\n0.5882353\n\n\nspec\nbinary\n0.9815951\n\n\nppv\nbinary\n0.7692308\n\n\nnpv\nbinary\n0.9580838\n\n\nmcc\nbinary\n0.6437748\n\n\nj_index\nbinary\n0.5698304\n\n\nbal_accuracy\nbinary\n0.7849152\n\n\ndetection_prevalence\nbinary\n0.0722222\n\n\nprecision\nbinary\n0.7692308\n\n\nrecall\nbinary\n0.5882353\n\n\nf_meas\nbinary\n0.6666667\n\n\n\n\n\n\n\nCodeplot_data &lt;- PredictionBestModel_f1|&gt; \n  select(.pred_Churned, Churn) |&gt; \n  mutate(Churn = factor(Churn, levels = c(\"Active\", \"Churned\"))) |&gt; \n  mutate(ProbBin = round(100 * .pred_Churned / 5) * 5) |&gt; \n  count(ProbBin, Churn) |&gt; \n  pivot_wider(names_from = Churn, values_from = n, values_fill = 0) |&gt; \n  pivot_longer(cols = c(\"Active\", \"Churned\"), names_to = \"Class\", values_to = \"Count\")\n\n\nggplot(plot_data, aes(x = factor(ProbBin), y = Count, fill = Class)) +\n  geom_col(position = \"stack\", width = 0.9) +\n  scale_fill_manual(values = c(\"Active\" = \"steelblue\", \"Churned\" = \"tomato\")) +\n  labs(\n    title = \"Predicted Churn Probability vs. Actual Class\",\n    x = \"Predicted Churn Probability (%)\",\n    y = \"Number of Users\",\n    fill = \"Actual Class\"\n  ) +\n  theme_minimal(base_size = 13)"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-random-forest/index.html#conclusion",
    "href": "posts/churn-prediction-tidymodels-random-forest/index.html#conclusion",
    "title": "Churn Prediction with Tidymodels - Part 2: Random Forest",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nThe Random Forest model demonstrated strong predictive performance in identifying customer churn. After tuning the model using ROC AUC and optimizing the classification threshold for F1, it achieved balanced and reliable results — with precision and recall both around 0.86 and an overall F1 score of 0.83. This means the model correctly identifies the majority of churners while maintaining a low rate of false positives.\nCompared to earlier approaches, such as KNN, Random Forest provided superior accuracy, stability, and interpretability through feature importance. Overall, the model offers a robust and actionable framework for predicting churn risk, enabling the business to focus retention efforts on the customers most likely to leave."
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html",
    "href": "posts/hierarchical-clustering-tidymodels/index.html",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "",
    "text": "Customer segmentation is one of the primary components of marketing strategy, informing promotional offers, personalized communications and audience signals on paid media platforms. In this study, we run a hierarchical cluster analysis using the Tidymodels package in R, to segment the customers of a real UK-based a online retail store."
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#data-description",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#data-description",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "\n1 Data Description",
    "text": "1 Data Description\nThe data set is downloaded from UCI Machine Learning Repository and is sourced from a 2012 academic paper on data mining by Chen et al 1. The company observed sells gifts for various occasions and the majority of its customers consist of wholesalers.\n\n\n\n\n\n\n\n\n\nVariable Name\nRole\nType\nDescription\nUnits\n\n\n\nInvoiceNo\nID\nCategorical\na 6-digit integral number uniquely assigned to each transaction. If this code starts with letter ‘c’, it indicates a cancellation\n\n\n\nStockCode\nID\nCategorical\na 5-digit integral number uniquely assigned to each distinct product\n\n\n\nDescription\nFeature\nCategorical\nproduct name\n\n\n\nQuantity\nFeature\nInteger\nthe quantities of each product (item) per transaction\n\n\n\nInvoiceDate\nFeature\nDate\nthe day and time when each transaction was generated\n\n\n\nUnitPrice\nFeature\nContinuous\nproduct price per unit\nsterling\n\n\nCustomerID\nFeature\nCategorical\na 5-digit integral number uniquely assigned to each customer\n\n\n\nCountry\nFeature\nCategorical\nthe name of the country where each customer resides"
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#load-packages-data",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#load-packages-data",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "\n2 Load Packages & Data",
    "text": "2 Load Packages & Data\nWe will use the Tidyverse package for data processing. The dataset is in a tidy format, with each variable represented as a column and each observation as a row.\n\nCodelibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidyclust)\nlibrary(modeldata)\nlibrary(factoextra)\nlibrary(cluster)\n\n\n\nCoderetail &lt;- readxl::read_xlsx(\"retail.xlsx\")\nhead(retail)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvoiceNo\nStockCode\nDescription\nQuantity\nInvoiceDate\nUnitPrice\nCustomerID\nCountry\n\n\n\n536365\n85123A\nWHITE HANGING HEART T-LIGHT HOLDER\n6\n2010-12-01 08:26:00\n2.55\n17850\nUnited Kingdom\n\n\n536365\n71053\nWHITE METAL LANTERN\n6\n2010-12-01 08:26:00\n3.39\n17850\nUnited Kingdom\n\n\n536365\n84406B\nCREAM CUPID HEARTS COAT HANGER\n8\n2010-12-01 08:26:00\n2.75\n17850\nUnited Kingdom\n\n\n536365\n84029G\nKNITTED UNION FLAG HOT WATER BOTTLE\n6\n2010-12-01 08:26:00\n3.39\n17850\nUnited Kingdom\n\n\n536365\n84029E\nRED WOOLLY HOTTIE WHITE HEART.\n6\n2010-12-01 08:26:00\n3.39\n17850\nUnited Kingdom\n\n\n536365\n22752\nSET 7 BABUSHKA NESTING BOXES\n2\n2010-12-01 08:26:00\n7.65\n17850\nUnited Kingdom\n\n\n\n\n\n\nFor data exploration purposes, I will be adding a sales variable to show the amount collected from each transaction in pounds.\n\nCoderetail &lt;- retail %&gt;%\n  mutate(Sales = UnitPrice * Quantity)"
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#data-exploration",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#data-exploration",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "\n3 Data Exploration",
    "text": "3 Data Exploration\nWe have 406829 observations stored in our data set, each representing the purchase or refund of a stock ID. There are 18287 unique customers across 37 countries. The data set covers a period from 01/12/2010 to 09/12/2011.\n\nCodeskimr::skim(retail)\n\n\nData summary\n\n\nName\nretail\n\n\nNumber of rows\n541909\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n4\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nInvoiceNo\n0\n1\n6\n7\n0\n25900\n0\n\n\nStockCode\n0\n1\n1\n12\n0\n4070\n0\n\n\nDescription\n1454\n1\n1\n35\n0\n4211\n0\n\n\nCountry\n0\n1\n3\n20\n0\n38\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nQuantity\n0\n1.00\n9.55\n218.08\n-80995.00\n1.00\n3.00\n10.00\n80995.0\n▁▁▇▁▁\n\n\nUnitPrice\n0\n1.00\n4.61\n96.76\n-11062.06\n1.25\n2.08\n4.13\n38970.0\n▁▇▁▁▁\n\n\nCustomerID\n135080\n0.75\n15287.69\n1713.60\n12346.00\n13953.00\n15152.00\n16791.00\n18287.0\n▇▇▇▇▇\n\n\nSales\n0\n1.00\n17.99\n378.81\n-168469.60\n3.40\n9.75\n17.40\n168469.6\n▁▁▇▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nInvoiceDate\n0\n1\n2010-12-01 08:26:00\n2011-12-09 12:50:00\n2011-07-19 17:17:00\n23260\n\n\n\n\nThere are some transactions with 0 sterling value, which may have been the result of an offer such as buy one, get one or a complementary addition to the order. The max and minimum quantity are respectively 80995 and -80995, suggesting there are substantial refunds among these transactions.\nThe dataset also has missing data in the Customer ID column.\n\nCodesummary(retail)\n\n  InvoiceNo          StockCode         Description           Quantity        \n Length:541909      Length:541909      Length:541909      Min.   :-80995.00  \n Class :character   Class :character   Class :character   1st Qu.:     1.00  \n Mode  :character   Mode  :character   Mode  :character   Median :     3.00  \n                                                          Mean   :     9.55  \n                                                          3rd Qu.:    10.00  \n                                                          Max.   : 80995.00  \n                                                                             \n  InvoiceDate                       UnitPrice           CustomerID    \n Min.   :2010-12-01 08:26:00.00   Min.   :-11062.06   Min.   :12346   \n 1st Qu.:2011-03-28 11:34:00.00   1st Qu.:     1.25   1st Qu.:13953   \n Median :2011-07-19 17:17:00.00   Median :     2.08   Median :15152   \n Mean   :2011-07-04 13:34:57.16   Mean   :     4.61   Mean   :15288   \n 3rd Qu.:2011-10-19 11:27:00.00   3rd Qu.:     4.13   3rd Qu.:16791   \n Max.   :2011-12-09 12:50:00.00   Max.   : 38970.00   Max.   :18287   \n                                                      NA's   :135080  \n   Country              Sales           \n Length:541909      Min.   :-168469.60  \n Class :character   1st Qu.:      3.40  \n Mode  :character   Median :      9.75  \n                    Mean   :     17.99  \n                    3rd Qu.:     17.40  \n                    Max.   : 168469.60  \n                                        \n\n\n“DOTCOM POSTAGE” is the top product in terms of overall sales, followed by “REGENCY CAKESTAND”\n\nCoderetail %&gt;%\n  group_by(Description)  %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  arrange(desc(Sales)) %&gt;%\n  slice_head(n=10)\n\n\n\n\nDescription\nSales\n\n\n\nDOTCOM POSTAGE\n206245.48\n\n\nREGENCY CAKESTAND 3 TIER\n164762.19\n\n\nWHITE HANGING HEART T-LIGHT HOLDER\n99668.47\n\n\nPARTY BUNTING\n98302.98\n\n\nJUMBO BAG RED RETROSPOT\n92356.03\n\n\nRABBIT NIGHT LIGHT\n66756.59\n\n\nPOSTAGE\n66230.64\n\n\nPAPER CHAIN KIT 50’S CHRISTMAS\n63791.94\n\n\nASSORTED COLOUR BIRD ORNAMENT\n58959.73\n\n\nCHILLI LIGHTS\n53768.06\n\n\n\n\n\n\nUK is the biggest market, followed by Netherlands.\n\nCoderetail %&gt;%\n  group_by(Country)  %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  arrange(desc(Sales)) %&gt;%\n  slice_head(n=10)\n\n\n\n\nCountry\nSales\n\n\n\nUnited Kingdom\n8187806.36\n\n\nNetherlands\n284661.54\n\n\nEIRE\n263276.82\n\n\nGermany\n221698.21\n\n\nFrance\n197403.90\n\n\nAustralia\n137077.27\n\n\nSwitzerland\n56385.35\n\n\nSpain\n54774.58\n\n\nBelgium\n40910.96\n\n\nSweden\n36595.91\n\n\n\n\n\n\nGlobal markets only account for 16% of total sales.\n\nCoderetail %&gt;%\n  mutate(Market = if_else(Country == \"United Kingdom\", \"UK\", \"Global\")) %&gt;%\n  group_by(Market) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Perc_of_Sales = Sales / sum(Sales)) %&gt;%\n  ggplot(aes(x = Market, y = Sales, fill = Market)) +\n  geom_col() +\n  geom_text(aes(label = percent(Perc_of_Sales, accuracy = 0.1)),\n            vjust = -0.5, size = 5) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Sales Distribution by Market\",\n    x = \"Market\",\n    y = \"Total Sales\",\n    fill = \"Market\"\n  ) +\n  theme_minimal(base_size = 14) \n\n\n\n\n\n\n\nLikewise, the number of customers in the UK are 10 times the number of customers in Global. Expectedly, the mean and median are quite far from each other, indicating the presence of large outliers.\n\nCoderetail %&gt;%\n  group_by(CustomerID, Country) %&gt;%\n  summarise(Sales_per_Customer = sum(Sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Market = if_else(Country == \"United Kingdom\", \"UK\", \"Global\")) %&gt;%\n  group_by(Market) %&gt;%\n  summarise(\n    Customers = n_distinct(CustomerID),\n    mean_sales_per_customer = mean(Sales_per_Customer, na.rm = TRUE),\n    median_sales_per_customer = median(Sales_per_Customer, na.rm = TRUE)\n  )\n\n\n\n\nMarket\nCustomers\nmean_sales_per_customer\nmedian_sales_per_customer\n\n\n\nGlobal\n423\n3561.510\n964.755\n\n\nUK\n3951\n2072.338\n627.130\n\n\n\n\n\n\n5% of customers comprise over 50% of sales in both regions.\n\nCoderetail %&gt;%\n  group_by(CustomerID, Country) %&gt;%\n  summarise(Sales_per_Customer = sum(Sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Market = if_else(Country == \"United Kingdom\", \"UK\", \"Global\")) %&gt;%\n  group_by(Market) %&gt;%\n  mutate(percentile = ntile(Sales_per_Customer, 100)) %&gt;%\n  summarise(top_5_share = sum(Sales_per_Customer[percentile &gt; 95]) / sum(Sales_per_Customer))\n\n\n\n\nMarket\ntop_5_share\n\n\n\nGlobal\n0.5711857\n\n\nUK\n0.5507178\n\n\n\n\n\n\nAccording to this boxplot, these are indeed a group of customers that deviate greatly from the median in terms of sales volume.\n\nCoderetail %&gt;%\n  group_by(CustomerID, Country) %&gt;%\n  summarise(Sales_per_Customer = sum(Sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Market = if_else(Country == \"United Kingdom\", \"UK\", \"Global\")) %&gt;%\n  ggplot(aes(x = Market, y = Sales_per_Customer, fill = Market)) +\n  geom_boxplot(alpha = 0.4) + \n  scale_y_log10(labels = comma) +\n  scale_size_continuous(range = c(1, 10), guide = \"none\") +\n  labs(\n    title = \"Customer Sales Distribution by Market\",\n    x = \"Market\",\n    y = \"Sales per Customer (log scale)\"\n  ) +\n  theme_minimal(base_size = 13)"
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#hierarchical-cluster-analysis",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#hierarchical-cluster-analysis",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "\n4 Hierarchical Cluster Analysis",
    "text": "4 Hierarchical Cluster Analysis\nData exploration showed that we have some very large wholesale customers that undoubtedly have great value to us. Still, there is room for further exploration via unsupervised learning, taking recency and frequency of of orders alongside order value to look for more granular segments in our customer base.\nData Preparation\nSince we want to analyze customers, we will be removing the observations with missing customer ID from the cluster data set. Around 100,000 rows are missing a customer ID, which may lead to valuable data loss. However, seeing there is no way of connecting those transactions to an individual, they will have to be removed.\nEach customer will be assigned RFM (recency, frequency, monetary) scores based on the recency of their last transaction, number of invoices generated and the volume of sales.\n\nCode# define analysis date (last date in dataset)\ncutoff_date &lt;- max(retail$InvoiceDate, na.rm = TRUE)\n\nrfm &lt;- retail %&gt;%\n  filter(!is.na(CustomerID))  %&gt;% # remove transactions missing customerID\n  group_by(CustomerID) %&gt;%\n  summarise(\n    recency = as.numeric(cutoff_date - max(InvoiceDate)), # number of days since last transaction\n    frequency = n_distinct(InvoiceNo),   # number of transactions\n    monetary = sum(Sales) # total spending\n  ) %&gt;%\n  ungroup()\n\n\nA quick look at histograms reveals all three of these RFM variables are right-skewed.\n\nCodehist(rfm$recency)\n\n\n\n\n\n\nCodehist(rfm$frequency)\n\n\n\n\n\n\nCodehist(rfm$monetary)\n\n\n\n\n\n\n\nWe normalize them by first reducing skewness with log transformation to compress large value and make data more symmetric. Then, we rescale them to ensure all variables are on a comparable scale, which is important for distance-based methods like hyerarchical clustering.\n\nCoderfm &lt;- rfm  %&gt;%\n  filter(monetary &gt; 0) %&gt;% # remove customers with negative monetary value\n  mutate(\n    recency = log1p(recency), # reduce skewness\n    frequency = log1p(frequency),\n    monetary = log1p(monetary)\n  ) %&gt;%\n  mutate(across(c(recency, frequency, monetary), scale)) #standardize mean=0, sd=1\n\n\nCreating Clusters\nThe Elbow Method shows k = 2 offers the most distinct clustering, after which there is a steady drop.\n\nCodefviz_nbclust(rfm[, c(\"recency\", \"frequency\", \"monetary\")],\n             FUN = hcut,              # hierarchical clustering\n             method = \"wss\") + \n  labs(subtitle = \"Elbow Method\")\n\n\n\n\n\n\n\n\nCodek_values &lt;- 1:10\n\nwss_values &lt;- sapply(k_values, function(k) {\n  model &lt;- hclust(dist(rfm[, c(\"recency\", \"frequency\", \"monetary\")]),\n                  method = \"ward.D2\")\n  groups &lt;- cutree(model, k)\n \n# Compute within-cluster sum of squares manually\n  sum(sapply(unique(groups), function(g) {\n    cluster_data &lt;- rfm[groups == g, c(\"recency\", \"frequency\", \"monetary\")]\n    sum(scale(cluster_data, scale = FALSE)^2)\n  }))\n})\n\nelbow_df &lt;- data.frame(k = k_values, WSS = wss_values)\n\nelbow_df\n\n\n\n\nk\nWSS\n\n\n\n1\n12948.000\n\n\n2\n6793.457\n\n\n3\n5620.338\n\n\n4\n4599.952\n\n\n5\n3820.183\n\n\n6\n3378.954\n\n\n7\n2986.388\n\n\n8\n2670.212\n\n\n9\n2528.867\n\n\n10\n2390.305\n\n\n\n\n\n\nSilhouette Method confirms k = 2 is ideal for forming distinct segments.\n\nCodefviz_nbclust(\n  rfm[, c(\"recency\", \"frequency\", \"monetary\")],\n  FUN = hcut,               # hierarchical clustering\n  method = \"silhouette\",    \n  hc_method = \"ward.D2\"\n) +\n  labs(title = \"Silhouette Method for Optimal Number of Clusters\")\n\n\n\n\n\n\n\n\nCode# Define the range of k values you want to test\nk_values &lt;- 2:6\n\n# Compute silhouette average width for each k\nsil_scores &lt;- sapply(k_values, function(k) {\n  model &lt;- hcut(rfm[, c(\"recency\", \"frequency\", \"monetary\")],\n                k = k, hc_method = \"ward.D2\")\n  model$silinfo$avg.width\n})\n\n# Put results in a tidy data frame\nsil_df &lt;- data.frame(k = k_values, silhouette = sil_scores)\nsil_df\n\n\n\n\nk\nsilhouette\n\n\n\n2\n0.4013994\n\n\n3\n0.3331469\n\n\n4\n0.2464909\n\n\n5\n0.2494433\n\n\n6\n0.2393687\n\n\n\n\n\n\nUsing Tidymodels, we cut the dendrogram into 2 final clusters with Ward’s method to produce compact, roughly spherical clusters. Tidyclust will treat the result as a partitioning model, meaning every observation belongs to exactly one cluster.\n\nCodeRecipeCustSeg &lt;- recipe(~ recency + frequency + monetary, rfm)\n\n\nModelDesignClust &lt;- hier_clust(num_clusters = 2,\n                            linkage_method = \"ward.D\") %&gt;% \n                 set_engine(\"stats\")  %&gt;% \n                 set_mode(\"partition\")\n\nWFModelClust &lt;- workflow()  %&gt;% \n             add_model(ModelDesignClust)  %&gt;% \n             add_recipe(RecipeCustSeg)  %&gt;% \n             fit(rfm)\n\nModelTrainedClust &lt;-  extract_fit_engine(WFModelClust) \n\nDataCustSegWithAssignm &lt;- extract_cluster_assignment(WFModelClust)  %&gt;%  cbind(rfm) \n\nDataCentroids &lt;-  extract_centroids(WFModelClust)\n\n\nResults\nThe clustering algorithm has bucketed customers into groups based on their RFM characteristics.\n\nCodefviz_cluster(\n  list(\n    data = as.matrix(rfm[, c(\"recency\", \"frequency\", \"monetary\")]),\n    cluster = as.factor(DataCustSegWithAssignm$.cluster)\n  ),\n  geom = \"point\",\n  ellipse.type = \"convex\",\n  palette = \"jco\",\n  ggtheme = theme_minimal(),\n  main = \"Customer Segments (Hierarchical Clustering, k = 2)\"\n)\n\n\n\n\n\n\n\nTheir characteristics can be summarized as below:\n\nCluster 1 (positive F & M, negative R) → frequent, recent, high spenders → “High-Value” customers.\nCluster 2 (negative F & M, positive R) → infrequent, older purchases, low spend → “Low-Value” customers.\n\n\nCodeDataCustSegWithAssignm %&gt;%\n  group_by(.cluster) %&gt;%\n  summarise(\n    mean_recency = mean(recency),\n    mean_frequency = mean(frequency),\n    mean_monetary = mean(monetary),\n    n_customers = n()\n  ) %&gt;%\n  mutate(Segment = if_else(.cluster == \"Cluster_1\", \"High-Value\", \"Low-Value\")) \n\n\n\n\n\n\n\n\n\n\n\n\n.cluster\nmean_recency\nmean_frequency\nmean_monetary\nn_customers\nSegment\n\n\n\nCluster_1\n-0.5658819\n0.7170695\n0.7075554\n2212\nHigh-Value\n\n\nCluster_2\n0.5946464\n-0.7535191\n-0.7435214\n2105\nLow-Value\n\n\n\n\n\n\n\nCodeDataCentroids %&gt;%\n  pivot_longer(cols = c(recency, frequency, monetary),\n               names_to = \"metric\", values_to = \"mean_value\") %&gt;%\n  ggplot(aes(x = metric, y = mean_value, fill = .cluster)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = round(mean_value, 2)),\n            position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +\n  scale_fill_manual(\n    values = c(\"Cluster_1\" = \"blue\", \"Cluster_2\" = \"orange\")  \n  ) +\n  labs(title = \"Average Scaled RFM Values by Cluster\",\n       x = \"RFM Metric\",\n       y = \"Scaled Mean Value\",\n       fill = \"Cluster\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\nThe radar chart visualizes the characteristics of these segments relative to the three variables and each other. We have cearly differentiated customer groups, the segmentation is strong.\nCluster 1’s polygon expands outward on Frequency and Monetary, and inward on Recency. These are customers with strong loyalty: Recent, frequent, high-spending.\nCluster 2’s polygon is the opposite, expanding towards Recency, indicating older purchases. Below average performance, less frequent, lower spending.\n\nCodelibrary(fmsb)\n\n# Convert to matrix format\nrfm_radar &lt;- as.data.frame(DataCentroids[, -1])\nrow.names(rfm_radar) &lt;- DataCentroids$.cluster\n\n# Add upper/lower bounds for fmsb\nrfm_radar &lt;- rbind(rep(2, 3), rep(-2, 3), rfm_radar)\n\nradarchart(rfm_radar,\n           axistype = 1,\n           pcol = c(\"blue\", \"orange\"),\n           plwd = 2,\n           plty = 1,\n           title = \"RFM Cluster Profiles (Scaled Values)\")\n\nlegend(\"topright\",\n       legend = rownames(rfm_radar)[-c(1, 2)],  # skip the first two scale rows\n       col = c(\"blue\", \"orange\"),\n       lty = 1,  # same line type as chart\n       lwd = 2,  # same line width as chart\n       title = \"Clusters\")\n\n\n\n\n\n\n\nAnalysis\nBy joining our cluster analysis table with the original customer dataset, we can obtain a list that can be used for marketing campaigns.\n\nCoderetail_with_clusters &lt;- retail %&gt;%\n  left_join(\n    DataCustSegWithAssignm %&gt;%\n      select(CustomerID, .cluster) %&gt;%        \n      rename(Cluster = .cluster),             \n    by = \"CustomerID\"\n  ) %&gt;%\n  mutate(\n    Segment = recode(Cluster,\n                     \"Cluster_1\" = \"High-Value\",\n                     \"Cluster_2\" = \"Low-Value\")\n  )\n\n\nThis also enables us to further examine our customer data, and see the sales metrics of the two segments.\nA quick look shows that the segments are not only distinct, but give us a very meaningful look into the each segment.\nHigh-Value Segment\nThis group likely includes wholesale or business buyers:\n\nThe average high-value customer spends almost 9× more than a low-value one.\nHigh frequency (≈8.5 purchases per customer) suggests strong loyalty or recurring B2B activity.\nSome very large spenders (wholesale clients) are pulling the average upward.\n\nLow-Value Segment\nThese are likely individual or casual buyers.\n\nAverage sales per customer are very low (~$370).\nPurchase frequency barely above 1. Mostly one-time or occasional buyers.\nMean and median are close, little skew. These are fairly consistent low-spenders.\nLifetime value will be highly limited.\n\n\nCodesegment_summary &lt;- retail_with_clusters %&gt;%\n  group_by(Segment, CustomerID) %&gt;%\n  summarise(\n    Sales_per_Customer = sum(Sales, na.rm = TRUE),\n    Frequency = n_distinct(InvoiceNo),\n    .groups = \"drop_last\"\n  ) %&gt;%\n  summarise(\n    Customers = n_distinct(CustomerID),\n    mean_sales_per_customer = mean(Sales_per_Customer, na.rm = TRUE),\n    median_sales_per_customer = median(Sales_per_Customer, na.rm = TRUE),\n    mean_frequency = mean(Frequency, na.rm = TRUE),\n    median_frequency = median(Frequency, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nsegment_summary \n\n\n\n\n\n\n\n\n\n\n\n\nSegment\nCustomers\nmean_sales_per_customer\nmedian_sales_per_customer\nmean_frequency\nmedian_frequency\n\n\n\nHigh-Value\n2212\n3405.6107\n1545.275\n8.522604\n5\n\n\nLow-Value\n2105\n370.5863\n300.920\n1.542993\n1\n\n\nNA\n56\n25615.2318\n-22.950\n67.857143\n1\n\n\n\n\n\n\nNA Values\nThere are Customer ID rows there are either have missing Customer ID (leading to data loss) or have negative or zero Sales value (cannot be considered a customer). Both occurances were excluded from the cluster analysis.\n\nCodena_segment &lt;- retail_with_clusters %&gt;%\n  filter(is.na(Segment))\n\nna_segment %&gt;% \n  group_by(CustomerID)  %&gt;% \n  summarise(\n    n_rows = n(),\n    unique_customers = n_distinct(CustomerID),\n    avg_sales = mean(Sales, na.rm = TRUE),\n    total_sales = sum(Sales, na.rm = TRUE)) %&gt;% \n  arrange(desc(n_rows))  %&gt;% \n  head()\n\n\n\n\nCustomerID\nn_rows\nunique_customers\navg_sales\ntotal_sales\n\n\n\nNA\n135080\n1\n10.717220\n1447682.12\n\n\n12607\n202\n1\n0.000000\n0.00\n\n\n18072\n36\n1\n0.000000\n0.00\n\n\n14557\n32\n1\n0.000000\n0.00\n\n\n16546\n31\n1\n-3.094516\n-95.93\n\n\n12454\n30\n1\n0.000000\n0.00"
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#conclusion",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#conclusion",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nHierarchical Clustering Analysis is a useful tool in segmenting customers based on shared characteristics, allowing marketers to tailor their strategy to the segments. Now that customers are assigned to clusters, the segment labels can be passed back to a CRM tool and an internal data warehouse for:\n\nSegment-based A/B testing\nCustomer lifetime value prediction\nMarketing automation targeting\n\nCluster 1 represents high-value customers who purchase more frequently, spend more, and buy more recently than average. Cluster 2 represents low-value or inactive customers with infrequent, lower-spend, and older purchase activity. This segmentation confirms a highly skewed customer base, where a smaller group of wholesale or loyal B2B buyers drive a large share of total revenue.\nTherefore, each should receive communications tailored to their profile. High-Value segment in particular should be analyzed more deeply and the marketing strategy should focus on relationship building. Wholesale customers are difficult to acquire yet bring in the most revenue, so the bulk of the marketing budget should be allocated to retaining these accounts.\nLow-Value customers don’t even make an average of 1 purchase per year and the purchase amount is significantly lower. Therefore, the communications should not be frequent and sales-focused. Some budget may be allocated to reactivation experiments for promising customers that may be small business or organizations.\n\n\n\n\n\n\n\nSegment\nStrategy\nRecommended Action\n\n\n\nHigh-Value\nRetention & relationship growth\nAccount-based marketing, exclusive offers, volume discounts, early access to new inventory.\n\n\nLow-Value\nLight engagement / reactivation\nSeasonal promotions, personalized email nurture flows, referral incentives.\n\n\n\nNext steps should focus on deepening these insights by overlaying additional attributes such as product category preferences, size of the account, buying cycles etc. This will help validate behavioral patterns and reveal sub-segments within each group."
  },
  {
    "objectID": "posts/hierarchical-clustering-tidymodels/index.html#footnotes",
    "href": "posts/hierarchical-clustering-tidymodels/index.html#footnotes",
    "title": "Hierarchical Cluster Analysis with Tidymodels on Retail Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nChen, D. (2015). Online Retail [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5BW33.↩︎"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html",
    "href": "posts/churn-prediction-tidymodels-knn/index.html",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "",
    "text": "This study aims to develop a customer churn prediction model for the fictional streaming service Skystream, using a synthetic dataset.\nSkystream is an emerging U.S.-based platform currently focusing its marketing efforts across eight states. The analysis employs a K-Nearest Neighbors (KNN) supervised learning algorithm to predict customer churn based on demographic and behavioral attributes within the dataset."
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#load-packages-data",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#load-packages-data",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n1 Load Packages & Data",
    "text": "1 Load Packages & Data\nWe will use the Tidyverse package for data processing. At first glance, the dataset appears to be in a tidy format, with each variable represented as a column and each observation as a row.\n\nCodelibrary(tidyverse)\nlibrary(DataExplorer)\nlibrary(patchwork)\nlibrary(GGally)\nlibrary(fastDummies)\nlibrary(snakecase)\nlibrary(tidymodels)\nlibrary(themis)\nlibrary(yardstick)\nlibrary(survival)\nlibrary(survminer)\nlibrary(paletteer)\n\nskystream &lt;- read_csv(file = \"skystream.csv\")\n\n\nWe have 900 observations and 14 variables. Several of these numeric and character variables should be converted to factors for analysis purposes. We will want to see the distribution of demographic and behavioral characteristics in our customer base.\nWe also have 883 missing values that we need to look into. The majority of the missing values are in the CancelDate variable, which is expected as active users are not meant to have a cancel date. The remaining 2 missing values are in the JoinDate variable.\n\nCodeskimr::skim(skystream)\n\n\nData summary\n\n\nName\nskystream\n\n\nNumber of rows\n900\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nGenre\n0\n1\n5\n18\n0\n7\n0\n\n\nState\n0\n1\n2\n2\n0\n9\n0\n\n\nSubscriptionTier\n0\n1\n5\n12\n0\n4\n0\n\n\nGender\n0\n1\n1\n1\n0\n3\n0\n\n\nDevicePreference\n0\n1\n5\n8\n0\n5\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\nJoinDate\n2\n1.00\n2025-01-02\n2025-03-30\n2025-02-06\n88\n\n\nCancelDate\n821\n0.09\n2025-03-05\n2025-10-30\n2025-04-26\n49\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nCustomerID\n0\n1\n461.61\n278.32\n1.00\n225.75\n450.50\n675.25\n1000.00\n▇▇▇▇▃\n\n\nAge\n0\n1\n39.61\n12.39\n18.00\n29.00\n38.00\n49.00\n68.00\n▇▇▇▅▃\n\n\nNumberMonthsActive\n0\n1\n4.10\n2.12\n1.00\n3.00\n4.00\n5.00\n12.00\n▅▇▃▁▁\n\n\nAvgSessionLength\n0\n1\n50.26\n20.61\n20.00\n33.00\n45.00\n65.00\n90.00\n▇▇▆▃▅\n\n\nMonthsSinceLastActivity\n0\n1\n0.78\n0.47\n0.00\n1.00\n1.00\n1.00\n3.00\n▃▇▁▁▁\n\n\nAvgWatchHoursPerMonth\n0\n1\n20.49\n9.02\n7.00\n13.00\n19.00\n29.00\n65.00\n▇▇▂▁▁\n\n\nRevenueYTD\n0\n1\n54.20\n40.90\n5.99\n19.99\n39.96\n79.95\n239.88\n▇▃▂▁▁\n\n\n\n\n\nTwo users have 4 NumberMonthsActive but not JoinDate or CancelDate, hence we aren’t able to trace back from CancelDate to fill in their JoinDate. However, we aren’t interested in JoinDate as a variable in our churn model so we not be removing these users with NA values from our data set.\n\nCodeskystream %&gt;%\n  filter(is.na(JoinDate))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nState\nSubscriptionTier\nAge\nGender\nJoinDate\nCancelDate\nNumberMonthsActive\nAvgSessionLength\nMonthsSinceLastActivity\nAvgWatchHoursPerMonth\nRevenueYTD\nDevicePreference\n\n\n\n714\nDocumentary\nTX\nFamily\n24\nF\nNA\nNA\n4\n75\n1\n29\n79.96\nSmart TV\n\n\n71\nChildren & Family\nWA\nFamily\n44\nM\nNA\nNA\n4\n78\n1\n28\n79.96\nLaptop\n\n\n\n\n\n\nChecking for unique values in each categorical variable, we find that the genre Drama is coded as Dramas for some observations, we need to recode these.\n\nCodeskystream %&gt;%\n  select(where(~ is.character(.x) || is.factor(.x))) %&gt;%\n  map(unique)\n\n$Genre\n[1] \"Horror\"             \"Action & Adventure\" \"Documentary\"       \n[4] \"Children & Family\"  \"Drama\"              \"Comedy\"            \n[7] \"Dramas\"            \n\n$State\n[1] \"IL\" \"TX\" \"FL\" \"GA\" \"WA\" \"NY\" \"CA\" \"CO\" \"AZ\"\n\n$SubscriptionTier\n[1] \"Premium\"      \"Family\"       \"Ad-Supported\" \"Basic\"       \n\n$Gender\n[1] \"M\" \"F\" \"O\"\n\n$DevicePreference\n[1] \"Mobile\"   \"Smart TV\" \"Tablet\"   \"Laptop\"   \"Mixed\""
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#data-wrangling",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#data-wrangling",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n2 Data Wrangling",
    "text": "2 Data Wrangling\nWe create a new variable for Churn based on whether the user has a CancelDate or not and clean up our Genre variable.\n\nCodeskystream &lt;- skystream %&gt;%\n  mutate(\n    Churn = if_else(is.na(CancelDate), 0L, 1L) #return 1 if there is a CancelDate\n  ) %&gt;%\n  relocate(Churn, .after = 1) %&gt;% #position it after column 1 \n  mutate(Genre = recode(Genre, \"Dramas\" = \"Drama\"), #replace Dramas with Drama\n         SubscriptionTier = factor(SubscriptionTier,\n                              levels = c(\"Ad-Supported\",\"Basic\", \"Premium\", \"Family\"), #relevel tiers\n                              ordered = TRUE))"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#data-exploration",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#data-exploration",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n3 Data Exploration",
    "text": "3 Data Exploration\nThere are users from 9 states in the data set, equally distributed, minus the AZ user base which seems to be extremely low. There might be a mistake in the records.\nFemale users form the majority of the user base, and the device preference is mostly mobile. Basic is the most preferred subscription tier, while Ad-Support is the least despite being the cheapest option. Users seem willing to pay more for the ad-free experience.\n\nCodedevice_plot &lt;- skystream %&gt;%\n  ggplot(aes(x = fct_infreq(DevicePreference))) +\n  geom_bar(fill = \"#7C873EFF\") +\n  labs(title = \"Device Preference\", x = \"Device\", y = \"Count\") +\n  theme_minimal() \n\nstate_plot &lt;- skystream %&gt;%\n  ggplot(aes(x = fct_infreq(State))) +\n  geom_bar(fill = \"#DB4743FF\") +\n  labs(title = \"State\", x = \"State\", y = \"Count\") +\n  theme_minimal() \n\ngender_plot &lt;- skystream %&gt;%\n  ggplot(aes(x = fct_infreq(Gender))) +\n  geom_bar(fill = \"#F5AF4DFF\") +\n  labs(title = \"Gender\", x = \"Gender\", y = \"Count\") +\n  theme_minimal() \n\ntier_plot &lt;- skystream %&gt;%\n  ggplot(aes(x = fct_infreq(SubscriptionTier))) +\n  geom_bar(fill = \"#5495CFFF\") +\n  labs(title = \"Subscription Tier\", x = \"Tier\", y = \"Count\") +\n  theme_minimal() \n\n\npatch_plot &lt;- state_plot + gender_plot + tier_plot + device_plot \n\npatch_plot + plot_annotation(\n  title = 'User Characteristics'\n) &\n  theme(\n    plot.title = element_text(hjust = 0.5) \n  )\n\n\n\n\n\n\n\nAge distribution is right skewed, with the majority of our user base falling between ages 20 to 40.\n\nCodeskystream %&gt;% \n  ggplot(aes(Age)) +\n  geom_histogram(\n    bins = 9,  \n    fill = \"#5495CFFF\",\n    color = \"white\"\n    ) +\n  labs(title = \"Age Distribution\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    panel.grid.major.x = element_blank()\n  )\n\n\n\n\n\n\n\nGenre preferences are quite evenly distributed within the user base.\n\n\n\n\n\n\n\n\nTiers by State reveals an interesting pattern. In TX and WA, Premium tier dominates while in CA and GA the leading tier is Family. CO and FL subscribe mainly to Basic tier, and IL and NY to Ad-Supported.\n\nCodeskystream %&gt;% \n  ggplot(aes(x = State, fill = SubscriptionTier)) +\n  geom_bar() +\n  labs(\n    title = \"Subscription Tiers by State\",\n    x = \"State\", y = \"Subscribers\", fill = \"Tier\"\n  ) +\n  theme_minimal(base_size = 13) +\n    scale_fill_paletteer_d(\"nationalparkcolors::Badlands\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n        legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16))\n\n\n\n\n\n\n\nThere is strong correlation between AvgWatchHoursPerMonth, AvgSessionLength, which seems logical in that users who spend more time on the platform per session, also spend more hours watching movies per month. It may also signal that increasing session duration is positive user behavior, instead of a negative behavior such as unproductive scrolling and browsing for content.\nNo strong correlation between continious variables and Churn.\n\nCodeskystream %&gt;%\n  select(Age, NumberMonthsActive, AvgWatchHoursPerMonth, AvgSessionLength, Churn) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\nNo strong correlation between categorical variables and Churn.\n\nCodelibrary(rcompanion)\n\ncramerV(table(skystream$Churn, skystream$Gender))        \n\nCramer V \n 0.02628 \n\nCodecramerV(table(skystream$Churn, skystream$Genre))          \n\nCramer V \n  0.1844 \n\nCodecramerV(table(skystream$Churn, skystream$SubscriptionTier)) \n\nCramer V \n  0.1822 \n\n\nLess than 10% of users have churned, out of 900 users.\n\nCodeskystream %&gt;%\n  count(Churn) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  ggplot(aes(x = factor(Churn), y = n, fill = factor(Churn))) +\n  geom_col() +\n  geom_text(aes(label = scales::percent(prop, accuracy = 1)),\n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"0\" = \"grey70\", \"1\" = \"#DB4743FF\")) +\n  scale_x_discrete(labels = c(\"0\" = \"No\", \"1\" = \"Yes\")) +\n  labs(x = \"Churn Status\",\n       y = \"Proportion of Users\",\n       title = \"Churn vs No Churn with Percentages\",\n       fill = \"Churn\") +\n  guides(fill = \"none\") \n\n\n\n\n\n\n\nThere is a significant drop among users after 9 months.\n\nCodefit &lt;- survfit(Surv(NumberMonthsActive, Churn) ~ 1, skystream)\n\nggsurvplot(fit, conf.int = TRUE,\n           xlab = \"Months\",\n           ylab = \"Proportion Still Active\",\n           title = \"User Retention Curve\")\n\n\n\n\n\n\n\nThe 9 month retention is similar across genders.\n\nCodefit_gender &lt;- survfit(Surv(NumberMonthsActive, Churn) ~ Gender, skystream)\n\nggsurvplot(fit_gender, skystream,\n           xlab = \"Months\",\n           ylab = \"Proportion Still Active\",\n           title = \"User Retention Curve by Gender\")\n\n\n\n\n\n\n\nHorror and Documentary fans show significant loss in retention over time. They might be losing interest due to a lack of new content in these genres.\n\nCodefit_genre &lt;- survfit(Surv(NumberMonthsActive, Churn) ~ Genre, skystream)\n\nggsurvplot(fit_genre, skystream,\n           xlab = \"Months\",\n           ylab = \"Proportion Still Active\",\n           title = \"User Retention Curve by Genre\")\n\n\n\n\n\n\n\nRetention is much stronger in higher tiers. Ad-supported and Basic seem to lose steam around 6 months.\n\nCodefit_tier &lt;- survfit(Surv(NumberMonthsActive, Churn) ~ SubscriptionTier, skystream)\n\nggsurvplot(fit_tier, skystream,\n           xlab = \"Months\",\n           ylab = \"Proportion Still Active\",\n           title = \"User Retention Curve by Tier\")"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#data-preparation-for-knn",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#data-preparation-for-knn",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n4 Data Preparation for KNN",
    "text": "4 Data Preparation for KNN\nAfter initial data exploration, we prepare data for KNN. We ensure that all continuous variables are numeric and we drop the variables that we will not be using in our model. RevenueYTD is dropped as it would be reduntant when we have both SubscriptionTier and NumberMonthsActive in the model, which directly determine this 3rd variable.\nWe convert the Churn variable, which indicates whether a customer has churned, into a factor so it can be treated as a categorical classification outcome.\nThe categorical variables are recoded as dummy variables so KNN can compute distance across categories.\n\nCode# Define columns by role/type\nnum_cols  &lt;- c(\"NumberMonthsActive\", \"AvgSessionLength\", \"Age\",\n               \"MonthsSinceLastActivity\", \"AvgWatchHoursPerMonth\")\n\ncat_cols  &lt;- c(\"SubscriptionTier\", \"Genre\", \"State\", \"Gender\", \"DevicePreference\")\n\nskystream_knn &lt;- skystream %&gt;%\n  #Make “Churned” the first level so tidymodels treats Churned as the positive class\n  mutate(Churn = factor(Churn, levels = c(1, 0), labels = c(\"Churned\", \"Active\"))) %&gt;%\n  \n  # Ensure numericals are numeric\n   mutate(across(all_of(num_cols), as.numeric)) %&gt;%\n  \n  # Remove columns not suitable as KNN features \n  select(-CustomerID, -JoinDate, -CancelDate, -RevenueYTD) %&gt;%\n  \n  # One-hot encode categoricals (drop first level to avoid redundancy)\n  fastDummies::dummy_cols(\n    select_columns = cat_cols,\n    remove_first_dummy = TRUE,\n    remove_selected_columns = TRUE\n  ) %&gt;%\n  \n  #Ensure column names are all in tidy format\n   rename_with(~ to_any_case(., case = \"upper_camel\"))"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#creating-the-model",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#creating-the-model",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n5 Creating the Model",
    "text": "5 Creating the Model\nWe set the seed using set.seed() to ensure the observations are random and consistent. We use initial_split() to divide the dataset, with 80% allocated for training and 20% for testing. Then, we use the strata = churned argument to ensure the class distribution of the churned variable is kept in both the training and testing sets. Finally, we extract the datasets for training and testing using split.\n\nCodeset.seed(867)\nSplit80 = initial_split(skystream_knn, prop = 0.80, strata = Churn)\n\nDataTrain &lt;- training(Split80) \nDataTest  &lt;- testing(Split80)  \n\n\nWe define the recipe for preprocessing. We specify the recipe with Churn ~ ., which means all other columns, such as Age and NumberMonthsActive, will be used to predict whether a customer churned.\nWe use step_normalize(all_predictors()) to scale all predictor variables and ensure they are all within the same scale.\nSince our 900 observation data is highly imbalanced, churned users comprising less than 10% of the user base, we use SMOTE from the themis package to create new synthetic samples of the minority class. SMOTE can create realistic “synthetic churn customers” so the KNN model doesn’t always default to predicting “Active.” It is less likely to overfit compared to naive oversampling in order to balance classes.\n\nCodeRecipe = recipe(Churn ~ ., data = DataTrain) |&gt;\n  step_normalize(all_predictors()) %&gt;%  # scale numeric features\n  step_smote(Churn)# apply SMOTE to balance churn vs active \n\nModelDesign = nearest_neighbor(neighbors=tune()) |&gt;\n               set_engine(\"kknn\") |&gt; \n               set_mode(\"classification\")\n\nTuneWFModel = workflow() |&gt; \n            add_recipe(Recipe) |&gt;\n            add_model(ModelDesign) \n\n\nTo avoid over-fitting, we are looking for the ideal number of neighbors (k) for our model to consider when classifying data points.To tune our model to the best value of k, we first create a hyper-parameter grid for tuning, a dataframe of possible k values (k = 1 to k = 15). For each 15 hyperparameters (k = 1 to k = 15), we will run 5 folds to cross-validate our data.\n\nCodeParGrid=data.frame(neighbors=c(1:15))\n\nset.seed(123)\nFoldsForTuning=vfold_cv(DataTrain, v=5,\n                        strata=Churn)\n\nTuneResults=tune_grid(TuneWFModel, resamples=FoldsForTuning,\n                      grid=ParGrid, metrics=metric_set(roc_auc, yardstick::accuracy, sens, spec, f_meas)) #specify yardstick package to avoide conflict with rcompanion\n\nautoplot(TuneResults)\n\n\n\n\n\n\n\nAfter running the best hyper parameters. We went ahead and tuned the results using the metric ROC AUC, which is meant to find the balance between true positive rate and false positive rate. We want to find the optimal point where we have the highest percentage of correctly identified churn cases against the lowest number of wrongly identified active users.\nBased on ROC AUC, the best value for the hyper-parameter is 11.\n\nCodeBestHyperParameter=select_best(TuneResults, metric=\"roc_auc\")\n\nprint(BestHyperParameter)\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1        11 Preprocessor1_Model11\n\n\nBased on the results of our Best Model, we see that 6 users from the data were a true positives, meaning they were correctly predicted to churn base on our variables given. We have a 35% true positive rate.\nTrue positive rate (also called sensitivity or recall) is the most important metric in churn prediction, given our goal is to correctly identify customers who will churn before they can do so. The model should be able to identify the highest number of churners correctly out of all those who actually end up churning churn.\nThis model was able to identify only 35% of the churners, and the precision is very low. Out of all the users model predicts will churn, only 20% actually churn, the number of false positives is very high. This could lead to an inefficient allocation of the retention budget.\n\nCodeWFModelBest=TuneWFModel |&gt;\n  finalize_workflow(BestHyperParameter) |&gt;\n  fit(DataTrain)\n\nPredictionBestModel=augment(WFModelBest, DataTest)\n\ncm &lt;- conf_mat(PredictionBestModel, truth=Churn,\n         estimate=.pred_class)\n\ncm\n\n          Truth\nPrediction Churned Active\n   Churned       6     24\n   Active       11    139\n\nCodecm %&gt;% summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.8055556\n\n\nkap\nbinary\n0.1532258\n\n\nsens\nbinary\n0.3529412\n\n\nspec\nbinary\n0.8527607\n\n\nppv\nbinary\n0.2000000\n\n\nnpv\nbinary\n0.9266667\n\n\nmcc\nbinary\n0.1614174\n\n\nj_index\nbinary\n0.2057019\n\n\nbal_accuracy\nbinary\n0.6028510\n\n\ndetection_prevalence\nbinary\n0.1666667\n\n\nprecision\nbinary\n0.2000000\n\n\nrecall\nbinary\n0.3529412\n\n\nf_meas\nbinary\n0.2553191\n\n\n\n\n\n\nWe need to refine our model further to get the best results out of KNN."
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#refining-the-model",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#refining-the-model",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n6 Refining the Model",
    "text": "6 Refining the Model\nHyperparameter Grid\nHyperparameter grids with k = [1:30] and k = [1:60] were also tested, which gave k = 11 (same as k = 15) and k = 43 as best hyperparameters. When k = 43 was used to tune the model, recall increased but precision dropped further.\nAs a result, we maintain k = 11 as a balanced hyperparameter, that also uses less computational power.\nThresholds\nAs we have an imbalanced data set, we tuned our hyperparameter to ROC AUC, settling on the k with the highest AUC. ROC AUC trains our model to be good at ranking positives ahead of negatives regardless of the threshold.\nNow we can move on to determining the best threshold to balance recall and precision.\nWe will scan a range of thresholds and pick the threshold that maximizes F1.\n\nCode# Try thresholds from 0.05 to 0.95\nths &lt;- seq(0.05, 0.95, by = 0.01)\n\nscan_f1 &lt;- map_dfr(ths, function(t) {\n  cls &lt;- factor(\n    ifelse(PredictionBestModel$.pred_Churned &gt;= t, \"Churned\", \"Active\"),\n    levels = c(\"Churned\",\"Active\")\n  )\n  tibble(\n    threshold = t,\n    f1 = f_meas_vec(PredictionBestModel$Churn, cls, beta = 1),\n    precision = precision_vec(PredictionBestModel$Churn, cls),\n    recall = sens_vec(PredictionBestModel$Churn, cls)\n  )\n})\n\n# Show the best threshold by F1\nbest_thresh_f1 &lt;- scan_f1 %&gt;%\n  slice_max(f1, n = 1) %&gt;%\n  pull(threshold)\n\nbest_thresh_f1 \n\n[1] 0.7\n\n\nIt appears that F1 peaks around t = 0.70. This increases our precision significantly while decreasing our recall.\n\nCodef1_plot &lt;- ggplot(scan_f1, aes(x = threshold, y = f1)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"F1 Score Across Thresholds\", y = \"F1 Score\", x = \"Threshold\")\n\n f1_plot2 &lt;- ggplot(scan_f1, aes(x = threshold, y = precision)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Precision Score Across Thresholds\", y = \"Precision Score\", x = \"Threshold\")\n\n f1_plot3 &lt;- ggplot(scan_f1, aes(x = threshold, y = recall)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Recall Score Across Thresholds\", y = \"Recall Score\", x = \"Threshold\")\n \n\n f1_plot | (f1_plot2 / f1_plot3)\n\n\n\n\n\n\n\nUsing 0.70 as a threshold, we achieve a more balanced model compared to the default threshold of 0.5, which we got after tuning our model for ROC AUC.\nPrecision is now at 0.35, and recall at 0.35.\n\nCodePredictionBestModel_f1 &lt;- PredictionBestModel %&gt;%\n  mutate(\n    .pred_class_opt = factor(\n      ifelse(.pred_Churned &gt;= best_thresh_f1, \"Churned\", \"Active\"),\n      levels = c(\"Churned\",\"Active\")\n    )\n  )\n\ncm_f1 &lt;- conf_mat(PredictionBestModel_f1, truth = Churn, estimate = .pred_class_opt)\n\ncm_f1\n\n          Truth\nPrediction Churned Active\n   Churned       6     11\n   Active       11    152\n\nCodecm_f1 %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.8777778\n\n\nkap\nbinary\n0.2854565\n\n\nsens\nbinary\n0.3529412\n\n\nspec\nbinary\n0.9325153\n\n\nppv\nbinary\n0.3529412\n\n\nnpv\nbinary\n0.9325153\n\n\nmcc\nbinary\n0.2854565\n\n\nj_index\nbinary\n0.2854565\n\n\nbal_accuracy\nbinary\n0.6427283\n\n\ndetection_prevalence\nbinary\n0.0944444\n\n\nprecision\nbinary\n0.3529412\n\n\nrecall\nbinary\n0.3529412\n\n\nf_meas\nbinary\n0.3529412\n\n\n\n\n\n\nCompared to the previous model, this model is better at eliminating false churn cases. However, we are also missing out on more true positives due to the increased threshold for churn prediction.\n\nCodecm_f1\n\n          Truth\nPrediction Churned Active\n   Churned       6     11\n   Active       11    152\n\nCodecm \n\n          Truth\nPrediction Churned Active\n   Churned       6     24\n   Active       11    139\n\n\nWe will refine further by changing weights.\nInverse Distance Weighting\nIn our initial KNN model design, every neighbor were counted equally by default (weight_func = “rectangular”). This only takes into account the class of nearby points when predicting, ignoring distance. Inverse distance weighting (“inv”) makes KNN more sensitive to local structure, reducing the impact of “distant” neighbors by giving neighbors closer to the query point higher weight.\nSimply using IDW with ROC, yielded higher recall than our first model.\n\nCodeModelDesign_inv=nearest_neighbor(neighbors=tune(), weight_func = \"inv\") |&gt;\n               set_engine(\"kknn\") |&gt; \n               set_mode(\"classification\")\n\nTuneWFModel_inv=workflow() |&gt; \n            add_recipe(Recipe) |&gt;\n            add_model(ModelDesign_inv) \n\nParGrid=data.frame(neighbors=c(1:15))\n\nset.seed(123)\nFoldsForTuning=vfold_cv(DataTrain, v=5,\n                        strata=Churn)\n\nTuneResults_inv=tune_grid(TuneWFModel_inv, resamples=FoldsForTuning,\n                      grid=ParGrid, metrics=metric_set(roc_auc, yardstick::accuracy, sens, spec, f_meas))\n\nBestHyperParameter_inv=select_best(TuneResults_inv, metric=\"roc_auc\")\n\nWFModelBest_inv=TuneWFModel_inv |&gt;\n  finalize_workflow(BestHyperParameter_inv) |&gt;\n  fit(DataTrain)\n\nPredictionBestModel_inv=augment(WFModelBest_inv, DataTest)\n\ncm_inv &lt;- conf_mat(PredictionBestModel_inv, truth=Churn,\n         estimate=.pred_class)\n\ncm_inv\n\n          Truth\nPrediction Churned Active\n   Churned       9     31\n   Active        8    132\n\nCodecm_inv %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.7833333\n\n\nkap\nbinary\n0.2112360\n\n\nsens\nbinary\n0.5294118\n\n\nspec\nbinary\n0.8098160\n\n\nppv\nbinary\n0.2250000\n\n\nnpv\nbinary\n0.9428571\n\n\nmcc\nbinary\n0.2386248\n\n\nj_index\nbinary\n0.3392277\n\n\nbal_accuracy\nbinary\n0.6696139\n\n\ndetection_prevalence\nbinary\n0.2222222\n\n\nprecision\nbinary\n0.2250000\n\n\nrecall\nbinary\n0.5294118\n\n\nf_meas\nbinary\n0.3157895\n\n\n\n\n\n\nSetting the threshold to 0.70, again increased precision but decreased recall.\n\nCodescan_f1_inv &lt;- map_dfr(ths, function(t) {\n  cls &lt;- factor(\n    ifelse(PredictionBestModel_inv$.pred_Churned &gt;= t, \"Churned\", \"Active\"),\n    levels = c(\"Churned\",\"Active\")\n  )\n  tibble(\n    threshold = t,\n    f1 = f_meas_vec(PredictionBestModel_inv$Churn, cls, beta = 1),\n    precision = precision_vec(PredictionBestModel_inv$Churn, cls),\n    recall = sens_vec(PredictionBestModel_inv$Churn, cls)\n  )\n})\n\nbest_thresh_f1_inv &lt;- scan_f1_inv %&gt;% # Show the best threshold by F1\n  slice_max(f1, n = 1) %&gt;%\n  pull(threshold)\n\nPredictionBestModel_f1_inv &lt;- PredictionBestModel_inv %&gt;%\n  mutate(\n    .pred_class_opt = factor(\n      ifelse(.pred_Churned &gt;= best_thresh_f1_inv, \"Churned\", \"Active\"),\n      levels = c(\"Churned\",\"Active\")\n    )\n  )\n\ncm_inv_f1 &lt;- conf_mat(PredictionBestModel_f1_inv, truth = Churn, estimate = .pred_class_opt)\n\ncm_inv_f1\n\n          Truth\nPrediction Churned Active\n   Churned       8     17\n   Active        9    146\n\nCodecm_inv_f1 %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.8555556\n\n\nkap\nbinary\n0.3025335\n\n\nsens\nbinary\n0.4705882\n\n\nspec\nbinary\n0.8957055\n\n\nppv\nbinary\n0.3200000\n\n\nnpv\nbinary\n0.9419355\n\n\nmcc\nbinary\n0.3097504\n\n\nj_index\nbinary\n0.3662938\n\n\nbal_accuracy\nbinary\n0.6831469\n\n\ndetection_prevalence\nbinary\n0.1388889\n\n\nprecision\nbinary\n0.3200000\n\n\nrecall\nbinary\n0.4705882\n\n\nf_meas\nbinary\n0.3809524\n\n\n\n\n\n\nManhattan Distance Weighting\nOur previous models used Euclidean distance by default (dist_power = 2), which is sensitive to large differences in any single feature since squaring exaggerates outliers.\nWe can change this to Manhattan distance, which is more robust to high-dimensional and sparse data (like one-hot encoded variables), since it just adds absolute differences without squaring them.\nOur dataset has lots of dummy variables and scaled numerics. Therefore, we may set our argument to favor Manhattan (dist_power = 1) to treat all feature differences more evenly.\nEven without optimized for F1 score, Manhattan Distance Weighting gives us the highest recall and precision we obtained so far.\n\nCodeModelDesign_man=nearest_neighbor(neighbors=tune(), weight_func = \"inv\", dist_power = 1) |&gt;\n               set_engine(\"kknn\") |&gt; \n               set_mode(\"classification\")\n\nTuneWFModel_man=workflow() |&gt; \n            add_recipe(Recipe) |&gt;\n            add_model(ModelDesign_man) \n\nParGrid=data.frame(neighbors=c(1:15))\n\n\nset.seed(123)\nFoldsForTuning=vfold_cv(DataTrain, v=5,\n                        strata=Churn)\n\nTuneResults_man=tune_grid(TuneWFModel_man, resamples=FoldsForTuning,\n                      grid=ParGrid, metrics=metric_set(roc_auc, yardstick::accuracy, sens, spec, f_meas))\n\nBestHyperParameter_man=select_best(TuneResults_man, metric=\"roc_auc\")\n\nWFModelBest_man=TuneWFModel_man |&gt;\n  finalize_workflow(BestHyperParameter_man) |&gt;\n  fit(DataTrain)\n\nPredictionBestModel_man=augment(WFModelBest_man, DataTest)\n\ncm_man &lt;- conf_mat(PredictionBestModel_man, truth=Churn,\n         estimate=.pred_class)\n\ncm_man\n\n          Truth\nPrediction Churned Active\n   Churned       8     14\n   Active        9    149\n\nCodecm_man %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.8722222\n\n\nkap\nbinary\n0.3399235\n\n\nsens\nbinary\n0.4705882\n\n\nspec\nbinary\n0.9141104\n\n\nppv\nbinary\n0.3636364\n\n\nnpv\nbinary\n0.9430380\n\n\nmcc\nbinary\n0.3434781\n\n\nj_index\nbinary\n0.3846987\n\n\nbal_accuracy\nbinary\n0.6923493\n\n\ndetection_prevalence\nbinary\n0.1222222\n\n\nprecision\nbinary\n0.3636364\n\n\nrecall\nbinary\n0.4705882\n\n\nf_meas\nbinary\n0.4102564\n\n\n\n\n\n\nAfter optimizing for the F1 score, we achieved 47% recall with 57% precision.\n\nCode# Show the best threshold by F1\nscan_f1_man &lt;- map_dfr(ths, function(t) {\n  cls &lt;- factor(\n    ifelse(PredictionBestModel_man$.pred_Churned &gt;= t, \"Churned\", \"Active\"),\n    levels = c(\"Churned\",\"Active\")\n  )\n  tibble(\n    threshold = t,\n    f1 = f_meas_vec(PredictionBestModel_man$Churn, cls, beta = 1),\n    precision = precision_vec(PredictionBestModel_man$Churn, cls),\n    recall = sens_vec(PredictionBestModel_man$Churn, cls)\n  )\n})\n\nbest_thresh_f1_man &lt;- scan_f1_man %&gt;% # Show the best threshold by F1\n  slice_max(f1, n = 1) %&gt;%\n  pull(threshold)\n\nbest_thresh_f1_man \n\n[1] 0.62 0.63\n\nCodePredictionBestModel_f1_man &lt;- PredictionBestModel_man %&gt;%\n  mutate(\n    .pred_class_opt = factor(\n      ifelse(.pred_Churned &gt;= best_thresh_f1_man, \"Churned\", \"Active\"),\n      levels = c(\"Churned\",\"Active\")\n    )\n  )\n\ncm_man_f1 &lt;- conf_mat(PredictionBestModel_f1_man, truth = Churn, estimate = .pred_class_opt)\n\ncm_man_f1 %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.9166667\n\n\nkap\nbinary\n0.4710031\n\n\nsens\nbinary\n0.4705882\n\n\nspec\nbinary\n0.9631902\n\n\nppv\nbinary\n0.5714286\n\n\nnpv\nbinary\n0.9457831\n\n\nmcc\nbinary\n0.4736616\n\n\nj_index\nbinary\n0.4337784\n\n\nbal_accuracy\nbinary\n0.7168892\n\n\ndetection_prevalence\nbinary\n0.0777778\n\n\nprecision\nbinary\n0.5714286\n\n\nrecall\nbinary\n0.4705882\n\n\nf_meas\nbinary\n0.5161290"
  },
  {
    "objectID": "posts/churn-prediction-tidymodels-knn/index.html#conclusion",
    "href": "posts/churn-prediction-tidymodels-knn/index.html#conclusion",
    "title": "Churn Prediction with Tidymodels - Part 1: K-Nearest Neighbors",
    "section": "\n7 Conclusion",
    "text": "7 Conclusion\nOur final KNN model predicts 47% of all churn cases correctly with 57% precision.\n\nCodecm_man_f1\n\n          Truth\nPrediction Churned Active\n   Churned       8      6\n   Active        9    157\n\nCodecm_man_f1 %&gt;%\n  summary()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.9166667\n\n\nkap\nbinary\n0.4710031\n\n\nsens\nbinary\n0.4705882\n\n\nspec\nbinary\n0.9631902\n\n\nppv\nbinary\n0.5714286\n\n\nnpv\nbinary\n0.9457831\n\n\nmcc\nbinary\n0.4736616\n\n\nj_index\nbinary\n0.4337784\n\n\nbal_accuracy\nbinary\n0.7168892\n\n\ndetection_prevalence\nbinary\n0.0777778\n\n\nprecision\nbinary\n0.5714286\n\n\nrecall\nbinary\n0.4705882\n\n\nf_meas\nbinary\n0.5161290\n\n\n\n\n\n\nThrough several optimizations, we transformed KNN into a model that provides practical business value for churn prediction. The initial version, using Euclidean distance and equal neighbor weighting, was prone to misclassifying churners because the majority “Active” class dominated.\nBy introducing inverse distance weighting, the model became more sensitive to customers whose behavior closely resembled churners, improving its ability to detect at-risk users. Switching to Manhattan distance further improved results by better handling the many categorical (dummy) features in our dataset, allowing the model to differentiate churners more effectively. Finally, instead of relying on the default 0.5 probability cutoff, we optimized the decision threshold for the F1 score, which balanced the need to catch more churners (recall) with the need to reduce false alarms (precision).\nTogether, these changes yielded a model that can more reliably flag customers with genuine churn risk, enabling more targeted and cost-effective retention campaigns.\nNext Steps\nLooking ahead, we recommend benchmarking this improved KNN against tree-based methods such as Random Forest. These algorithms often outperform KNN on imbalanced, high-dimensional data and may further increase recall without sacrificing precision. Incorporating cost-sensitive evaluation (penalizing missed churns more heavily) and feature engineering to capture behavioral trends (e.g., declining engagement over time) would also align the modeling process more closely with business objectives. This will ensure that our churn detection framework continues to evolve into a robust, actionable tool for customer retention strategy."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Hierarchical Cluster Analysis with Tidymodels on Retail Data\n\n\n\n\n\n\nclustering\n\n\ncustomer segmentation\n\n\nexploratory data analysis\n\n\n\nHierarchical Cluster Analysis is conducted on UK-based online retail store data to segment customers based on recency, frequency and monetary metrics.\n\n\n\n\n\nCeren Unal\n\n\n\n\n\n\n\n\n\n\n\n\nChurn Prediction with Tidymodels - Part 1: K-Nearest Neighbors\n\n\n\n\n\n\nclassification\n\n\nexploratory data analysis\n\n\n\nThe first part of the Churn Prediction project explores the customer data set and builds a baseline model using the K-Nearest Neighbors algorithm.\n\n\n\n\n\nCeren Unal\n\n\n\n\n\n\n\n\n\n\n\n\nChurn Prediction with Tidymodels - Part 2: Random Forest\n\n\n\n\n\n\nclassification\n\n\n\nThe second part of the Churn Prediction project builds a Random Forest model using that outperforms the baseline model.\n\n\n\n\n\nCeren Unal\n\n\n\n\n\n\nNo matching items"
  }
]