{
  "hash": "adb64b5a152dd366a86eff8bdd6e5854",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Churn Prediction with Tidymodels - Part 2: Random Forest\"\nauthor: Ceren Unal\n#date:\ncategories: [classification]\nimage: image-man-watching-tv.png\ndescription: The second part of the Churn Prediction project builds a Random Forest model using that outperforms the baseline model.\ntoc: true\ntoc-title: Content\ntoc-location: right\nnumber-sections: true\nnumber-depth: 2\nsmooth-scroll: true\ndf-print: kable\ncode-fold: true\ncode-tools: true\ncode-overflow: wrap \ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\nhighlight-style: pygments\ncode-link: true\nexecute:\n  warning: false\n  message: false\n#comments:\n#  hypothesis: true\n---\n\n\n\n*This study aims to develop a customer churn prediction model for the fictional streaming service Skystream, using a synthetic dataset.*\n\nIn Part 1, I developed a benchmark churn prediction model for Skystream using the K-Nearest Neighbors (KNN) algorithm. The KNN model correctly identified 47% of all churn cases, achieving a precision of 57%.\n\nIn Part 2 the goal is to build a Random Forest model that outperforms KNN on imbalanced, high-dimensional data, with a focus on improving both recall and precision.\n\n## Load Packages & Data\n\nWe will be using the Tidyverse package to process our data set. As in Part 1, I will clean up the incorrect namings in the Genre variable and create a Churn variable based on CancelDate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(DataExplorer)\n\nskystream <- read_csv(file = \"skystream.csv\")\n\nskystream <- skystream|> \n  mutate(\n    churn = if_else(is.na(CancelDate), 0L, 1L) #return 1 if there is a CancelDate\n  ) %>%\n  relocate(churn, .after = 1) |>  #position it after column 1 \n  mutate(Genre = recode(Genre, \"Dramas\" = \"Drama\")) #replace Dramas with Drama\n\nhead(skystream)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| CustomerID| churn|Genre              |State |SubscriptionTier | Age|Gender |JoinDate   |CancelDate | NumberMonthsActive| AvgSessionLength| MonthsSinceLastActivity| AvgWatchHoursPerMonth| RevenueYTD|DevicePreference |\n|----------:|-----:|:------------------|:-----|:----------------|---:|:------|:----------|:----------|------------------:|----------------:|-----------------------:|---------------------:|----------:|:----------------|\n|          8|     0|Horror             |IL    |Premium          |  42|M      |2025-01-29 |NA         |                 12|               55|                       0|                    65|     191.88|Mobile           |\n|          2|     0|Action & Adventure |TX    |Family           |  39|F      |2025-01-04 |NA         |                 12|               70|                       0|                    58|     239.88|Smart TV         |\n|         59|     0|Documentary        |FL    |Family           |  48|M      |2025-01-03 |NA         |                 12|               88|                       0|                    57|     239.88|Smart TV         |\n|         11|     0|Action & Adventure |TX    |Family           |  30|O      |2025-01-06 |NA         |                 12|               74|                       0|                    55|     239.88|Smart TV         |\n|         75|     0|Horror             |IL    |Family           |  28|M      |2025-01-06 |NA         |                 12|               90|                       0|                    55|     239.88|Tablet           |\n|         35|     0|Children & Family  |GA    |Family           |  41|M      |2025-01-02 |NA         |                 12|               88|                       0|                    55|     239.88|Mobile           |\n\n</div>\n:::\n:::\n\n\n\n## Data Preparation for Random Forest\n\nWe'll prepare our data for Random Forest by ensuring that all continuous variables are numeric and we drop the variables that we won't be using in our model. RevenueYTD is dropped as it would be reduntant when we have both SubscriptionTier and NumberMonthsActive in the model, which directly determine this 3rd variable.\n\nWe'll convert the Churn variable, which indicates whether a customer has churned, into a factor so it can be treated as a categorical classification outcome.\n\nThe categorical variables will be recoded as dummy variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastDummies)\nlibrary(snakecase)\n\n# Define columns by role/type\nnum_cols  <- c(\"NumberMonthsActive\", \"AvgSessionLength\", \"Age\",\n               \"MonthsSinceLastActivity\", \"AvgWatchHoursPerMonth\")\n\ncat_cols  <- c(\"SubscriptionTier\", \"Genre\", \"State\", \"Gender\", \"DevicePreference\")\n\nskystream_rf <- skystream |> \n  #Make “Churned” the first level so tidymodels treats Churned as the positive class\n  mutate(churn = factor(churn, levels = c(1, 0), labels = c(\"Churned\", \"Active\"))) |> \n  \n  # Ensure numericals are numeric\n   mutate(across(all_of(num_cols), as.numeric)) |> \n  \n  # Remove columns not suitable as KNN features \n  select(-CustomerID, -JoinDate, -CancelDate, -RevenueYTD) |> \n  \n  # One-hot encode categoricals (drop first level to avoid redundancy)\n  fastDummies::dummy_cols(\n    select_columns = cat_cols,\n    remove_first_dummy = TRUE,\n    remove_selected_columns = TRUE\n  ) %>%\n  \n  #Ensure column names are all in tidy format\n   rename_with(~ to_any_case(., case = \"upper_camel\")) \n```\n:::\n\n\n\n## Creating the Model\n\nWe set the seed to 867, the same as KNN, to ensure the observations are random and consistent across both models. We use `initial_split()` to divide the dataset, with 80% allocated for training and 20% for testing. Then, we use the `strata = churned` argument to ensure the class distribution of the churned variable is kept in both the training and testing sets. Finally, we extract the datasets for training and testing using split.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(themis)\n\nset.seed(867)\nSplit80  <- initial_split(skystream_rf, prop = 0.80, strata = Churn)\n\nDataTrain <- training(Split80) \nDataTest  <- testing(Split80)  \n\nhead(DataTrain)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Churn  | Age| NumberMonthsActive| AvgSessionLength| MonthsSinceLastActivity| AvgWatchHoursPerMonth| SubscriptionTierBasic| SubscriptionTierFamily| SubscriptionTierPremium| GenreChildrenFamily| GenreComedy| GenreDocumentary| GenreDrama| GenreHorror| StateCa| StateCo| StateFl| StateGa| StateIl| StateNy| StateTx| StateWa| GenderM| GenderO| DevicePreferenceMixed| DevicePreferenceMobile| DevicePreferenceSmartTv| DevicePreferenceTablet|\n|:------|---:|------------------:|----------------:|-----------------------:|---------------------:|---------------------:|----------------------:|-----------------------:|-------------------:|-----------:|----------------:|----------:|-----------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|---------------------:|----------------------:|-----------------------:|----------------------:|\n|Active |  42|                 12|               55|                       0|                    65|                     0|                      0|                       1|                   0|           0|                0|          0|           1|       0|       0|       0|       0|       1|       0|       0|       0|       1|       0|                     0|                      1|                       0|                      0|\n|Active |  39|                 12|               70|                       0|                    58|                     0|                      1|                       0|                   0|           0|                0|          0|           0|       0|       0|       0|       0|       0|       0|       1|       0|       0|       0|                     0|                      0|                       1|                      0|\n|Active |  48|                 12|               88|                       0|                    57|                     0|                      1|                       0|                   0|           0|                1|          0|           0|       0|       0|       1|       0|       0|       0|       0|       0|       1|       0|                     0|                      0|                       1|                      0|\n|Active |  30|                 12|               74|                       0|                    55|                     0|                      1|                       0|                   0|           0|                0|          0|           0|       0|       0|       0|       0|       0|       0|       1|       0|       0|       1|                     0|                      0|                       1|                      0|\n|Active |  28|                 12|               90|                       0|                    55|                     0|                      1|                       0|                   0|           0|                0|          0|           1|       0|       0|       0|       0|       1|       0|       0|       0|       1|       0|                     0|                      0|                       0|                      1|\n|Active |  41|                 12|               88|                       0|                    55|                     0|                      1|                       0|                   1|           0|                0|          0|           0|       0|       0|       0|       1|       0|       0|       0|       0|       1|       0|                     0|                      1|                       0|                      0|\n\n</div>\n:::\n:::\n\n\n\nAs with KNN, we specify the recipe with Churn \\~ ., which means all other columns, such as Age and NumberMonthsActive, will be used to predict whether a customer churned.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRecipeRF <- recipe(Churn~., data=DataTrain)\n```\n:::\n\n\n\nThe model design specifies `min_n` and `mtry` as tunable hyperparameters and sets the number of trees to 2,000 for increased stability. The model is configured to run using the `\"ranger\"` engine and the `importance = \"impurity\"` argument enables the calculation of variable importance scores, helping identify which features contribute most to churn prediction. Setting `probability = TRUE` ensures that class probabilities are returned (not just hard predictions), which is required for ROC AUC and threshold optimization. Finally, `num.threads = detectCores()` allows the model to use all available processor cores to speed up computation, and `set_mode(\"classification\")` confirms the task is binary classification (churn vs. active).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parallel)\nModelDesignRandFor <- rand_forest(\n  min_n=tune(), \n  mtry=tune(), \n  trees=2000) |> \n  set_engine(\"ranger\", \n             importance = \"impurity\", #generate variable importance plots\n             probability = TRUE, #generate class probabilities\n             num.threads=detectCores()) |> #set as number of cores of the executing computer\n  set_mode(\"classification\")\n\nset.seed(123)\nWfModelRF=workflow() |>                             \n           add_recipe(RecipeRF) |> \n           add_model(ModelDesignRandFor) \n```\n:::\n\n\n\nWe'll tune `min_n` and `mtry` using a regular tuning grid created with `grid_regular()`, which generates 36 combinations of `mtry` (number of predictors sampled at each tree split) and `min_n` (minimum node size), evenly spaced across defined ranges. The `finalize(mtry(), ...)` function ensures that `mtry` adapts to the actual number of predictors in the training data.\n\nCross-validation is set up using `vfold_cv()` with 5 folds, stratified by the target variable `Churn` to preserve class balance in each fold. The `tune_grid()` function evaluates each hyperparameter combination using cross-validation and saves the class probabilities for later threshold tuning. A comprehensive metric set is used—including ROC AUC, accuracy, sensitivity, specificity, and F1 score—to assess both overall and class-specific performance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_grid <- grid_regular(\n  finalize(mtry(), DataTrain |> select(-Churn)),  # mtry depends on predictors\n  min_n(range = c(2L, 20L)),\n  levels = c(6, 6)   #36 configs\n)\n\nset.seed(123)\n\nFoldsForTuningRF=vfold_cv(DataTrain, v=5, strata=Churn)\n\nrf_tune <- tune_grid(\n  WfModelRF,\n  resamples = FoldsForTuningRF,\n  grid = rf_grid,\n  metrics = metric_set(roc_auc, accuracy, sens, spec, f_meas),\n  control = control_grid(save_pred = TRUE)\n)\n\nautoplot(rf_tune)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nThe best hyperparameters are 16 for `mtry` and 9 for `min_n` , based on the ROC AUC metric finds the optimal balance between true positive rate and false positive rate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rf <- select_best(rf_tune, metric = \"roc_auc\")\n\nbest_rf\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| mtry| min_n|.config               |\n|----:|-----:|:---------------------|\n|   11|     9|Preprocessor1_Model15 |\n\n</div>\n:::\n:::\n\n\n\nAfter running the best hyper parameters, we go ahead and take a look at our confusion matrix to assess model performance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWFModelBest=WfModelRF |>\n  finalize_workflow(best_rf) |>\n  fit(DataTrain)\n\nPredictionBestModel=augment(WFModelBest, DataTest)\n\ncm_rf <- conf_mat(PredictionBestModel, truth=Churn,\n         estimate=.pred_class)\n\nlibrary(yardstick)\n\ncm_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction Churned Active\n   Churned       9      1\n   Active        8    162\n```\n\n\n:::\n\n```{.r .cell-code}\ncm_rf %>% summary()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric              |.estimator | .estimate|\n|:--------------------|:----------|---------:|\n|accuracy             |binary     | 0.9500000|\n|kap                  |binary     | 0.6415929|\n|sens                 |binary     | 0.5294118|\n|spec                 |binary     | 0.9938650|\n|ppv                  |binary     | 0.9000000|\n|npv                  |binary     | 0.9529412|\n|mcc                  |binary     | 0.6680751|\n|j_index              |binary     | 0.5232768|\n|bal_accuracy         |binary     | 0.7616384|\n|detection_prevalence |binary     | 0.0555556|\n|precision            |binary     | 0.9000000|\n|recall               |binary     | 0.5294118|\n|f_meas               |binary     | 0.6666667|\n\n</div>\n:::\n:::\n\n\n\nRandom Forest is already outperforming KNN, with 86% recall and 86% precision, compared to 47% recall and 57% precision.\n\nVariable importance in our model reveals that `MonthsSinceLastActivity` was the biggest predictor of `Churn`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\nrf_fit <- WFModelBest |>\n          extract_fit_parsnip()\nvip(rf_fit$fit, num_features = 25)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Refining the Model\n\n### **Thresholds**\n\nTo further improve the recall and precision of our model, we'll try adjusting our threshold.\n\nAfter scanning a range of thresholds we find that the range of values from 0.47 to 0.5 maximizes F1 score, so the default threshold that we initially tuned our model to already maximized F1 score.\n\nThis is evident in that when we drop the threshold to 0.48, the recall and precision (along with all other metrics) remain the same.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Try thresholds from 0.05 to 0.95\nths <- seq(0.05, 0.95, by = 0.01)\n\nscan_f1 <- map_dfr(ths, function(t) {\n  cls <- factor(\n    ifelse(PredictionBestModel$.pred_Churned >= t, \"Churned\", \"Active\"),\n    levels = c(\"Churned\",\"Active\")\n  )\n  tibble(\n    threshold = t,\n    f1 = f_meas_vec(PredictionBestModel$Churn, cls, beta = 1),\n    precision = precision_vec(PredictionBestModel$Churn, cls),\n    recall = sens_vec(PredictionBestModel$Churn, cls)\n  )\n})\n\n# Show the best threshold by F1\nbest_thresh_f1 <- scan_f1 |>\n  slice_max(f1, n = 1)  |>\n  pull(threshold)\n\nbest_thresh_f1 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.25 0.26 0.27\n```\n\n\n:::\n\n```{.r .cell-code}\nPredictionBestModel_f1 <- PredictionBestModel  |>\n  mutate(\n    .pred_class_opt = factor(\n      ifelse(.pred_Churned >= 0.48, \"Churned\", \"Active\"),\n      levels = c(\"Churned\",\"Active\")\n    )\n  )\n\ncm_rf_f1 <- conf_mat(PredictionBestModel_f1, truth = Churn, estimate = .pred_class_opt)\n\ncm_rf_f1  |>\n  summary()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric              |.estimator | .estimate|\n|:--------------------|:----------|---------:|\n|accuracy             |binary     | 0.9444444|\n|kap                  |binary     | 0.6369504|\n|sens                 |binary     | 0.5882353|\n|spec                 |binary     | 0.9815951|\n|ppv                  |binary     | 0.7692308|\n|npv                  |binary     | 0.9580838|\n|mcc                  |binary     | 0.6437748|\n|j_index              |binary     | 0.5698304|\n|bal_accuracy         |binary     | 0.7849152|\n|detection_prevalence |binary     | 0.0722222|\n|precision            |binary     | 0.7692308|\n|recall               |binary     | 0.5882353|\n|f_meas               |binary     | 0.6666667|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_data <- PredictionBestModel_f1|> \n  select(.pred_Churned, Churn) |> \n  mutate(Churn = factor(Churn, levels = c(\"Active\", \"Churned\"))) |> \n  mutate(ProbBin = round(100 * .pred_Churned / 5) * 5) |> \n  count(ProbBin, Churn) |> \n  pivot_wider(names_from = Churn, values_from = n, values_fill = 0) |> \n  pivot_longer(cols = c(\"Active\", \"Churned\"), names_to = \"Class\", values_to = \"Count\")\n\n\nggplot(plot_data, aes(x = factor(ProbBin), y = Count, fill = Class)) +\n  geom_col(position = \"stack\", width = 0.9) +\n  scale_fill_manual(values = c(\"Active\" = \"steelblue\", \"Churned\" = \"tomato\")) +\n  labs(\n    title = \"Predicted Churn Probability vs. Actual Class\",\n    x = \"Predicted Churn Probability (%)\",\n    y = \"Number of Users\",\n    fill = \"Actual Class\"\n  ) +\n  theme_minimal(base_size = 13)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Conclusion\n\nThe Random Forest model demonstrated strong predictive performance in identifying customer churn. After tuning the model using ROC AUC and optimizing the classification threshold for F1, it achieved balanced and reliable results — with precision and recall both around 0.86 and an overall F1 score of 0.83. This means the model correctly identifies the majority of churners while maintaining a low rate of false positives.\n\nCompared to earlier approaches, such as KNN, Random Forest provided superior accuracy, stability, and interpretability through feature importance. Overall, the model offers a robust and actionable framework for predicting churn risk, enabling the business to focus retention efforts on the customers most likely to leave.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}